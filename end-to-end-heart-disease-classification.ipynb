{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting heart disease using machine learning\n",
    "\n",
    "This notebook looks into using various Python-based machine learning and data science libraries in\n",
    "an attempt to build machine learning model capable of predicting whether or not someone has heart\n",
    "disease based on their medical attributes.\n",
    "\n",
    "We're going to take the following approach:\n",
    "1. Problem definition\n",
    "2. Data\n",
    "3. Evaluation\n",
    "4. Features\n",
    "5. Modelling\n",
    "6. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Problem Definition**\n",
    "\n",
    "In a statement,\n",
    "> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n",
    "\n",
    "**2. Data**\n",
    "\n",
    "The original data came from Cleaveland database from the UCI Machine Learning Repository.\n",
    "https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "There is also a version of it available on Kaggle. https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "\n",
    "**3. Evaluation**\n",
    "\n",
    "> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue the project.\n",
    "\n",
    "**4. Features**\n",
    "This is where you'll get different information about each of the features in your data.\n",
    "\n",
    "Data Dictionary:\n",
    "\n",
    "* age: age in years\n",
    "* sex: (1 = male; 0 = female)\n",
    "* cp: chest pain type\n",
    "    0: Typical angina: chest pain related decrease blood supply to the sugar\n",
    "    1: Atypical angina: chest pain not related to heart\n",
    "    2: Non-anginal pain: typically esophaegal spasma (non heart related)\n",
    "    3: Asymptomatic: chest pain not showing signs of disease\n",
    "* trestbps: resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is             typically clause for concern\n",
    "* chol: serum cholestoral in mg/dl\n",
    "            * serum = LDL + HDL + .2 * triglycerides\n",
    "            * above 200 is cause for concern\n",
    "* fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "            * '>126' mg/dl signals diabetes\n",
    "* restecg: resting electrocardiographic results (values 0,1,2)\n",
    "            * 0: Nothing to note\n",
    "            * 1: ST-T Wave Abnormality\n",
    "            * 2: Possible or definite left ventricular hypertrophy\n",
    "* thalach: maximum heart rate achieved\n",
    "* exang: exercise induced angina (1 = yes; 0 = no)\n",
    "* oldpeak: ST depression induced by exercise relative to rest\n",
    "* slope: the slope of the peak exercise ST segment\n",
    "            * Value 1: upsloping\n",
    "            * Value 2: flat\n",
    "            * Value 3: downsloping\n",
    "* ca: number of major vessels (0-3) colored by flourosopy\n",
    "* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "* target: 1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the tools\n",
    "\n",
    "We're going to use Pandas, matplotlib and Numpy for data anlaysis and manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the tools we need\n",
    "\n",
    "# Regular EDA(Exploratory Data Analysis) and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# we want our plots to appear inside the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Models from scikit-learn for Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart-disease.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration (EDA)\n",
    "\n",
    "The goal here is to find out more about the data and become a subject matter expert on the dataset\n",
    "you are working with.\n",
    "\n",
    "1. What questions are you trying to solve?\n",
    "2. What kind of data do we have and how do we treat different types?\n",
    "3. What's missing from the data and how do you deal with it?\n",
    "4. Where are the outliers and why should you care about them?\n",
    "5. How can you add, change or remove features to get more out of your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts().plot.bar(color=[\"salmon\", \"lightblue\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease Frequency according to sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare target with sex\n",
    "pd.crosstab(df.target, df.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of crosstab\n",
    "pd.crosstab(df.target, df.sex).plot.bar(figsize=(10,6),\n",
    "                                        color=[\"salmon\",\"lightblue\"]);\n",
    "plt.title(\"Heart Disease frequency as per sex\")\n",
    "plt.xlabel(\"0: No Disease, 1: Disease\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend([\"Female\",\"Male\"]);\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age vs. Max Heart Rate for Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another figure\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Scatter with positive examples\n",
    "plt.scatter(df.age[df.target==1],\n",
    "            df.thalach[df.target==1],\n",
    "            c=\"salmon\");\n",
    "\n",
    "# Scatter with negative examples\n",
    "plt.scatter(df.age[df.target==0],\n",
    "            df.thalach[df.target==0],\n",
    "            c=\"darkblue\")\n",
    "\n",
    "# Add some helpful info\n",
    "plt.title(\"Heart Disease in function of Age and Max Heart Rate\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Max Heart Rate\")\n",
    "plt.legend([\"Disease\",\"No Disease\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the age column with a histogram\n",
    "df.age.plot.hist(color=\"red\", rwidth=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease Frequency per Chest Pain Type\n",
    "\n",
    "3. cp- chest pain type\n",
    "\n",
    "    * 0: Typical angina: chest pain related decrease blood supply to the sugar.\n",
    "    * 1: Atypical angina: chest pain not related to heart.\n",
    "    * 2: Non-anginal pain: typically esophaegal spasma (non heart related)\n",
    "    * 3: Asymptomatic: chest pain not showing signs of disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.cp, df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the crosstab more visual\n",
    "pd.crosstab(df.cp,df.target).plot(kind=\"bar\",\n",
    "                                  figsize=(10,6),\n",
    "                                  color=[\"red\",\"darkblue\"])\n",
    "\n",
    "# Add some communication\n",
    "plt.title(\"Heart Disease Frequency per Chest Pain Type\")\n",
    "plt.xlabel(\"Chest Pain Type\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend([\"No Disease\",\"Disease\"])\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Correlation matrix\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make our correlation matrix a little prettier\n",
    "corr_matrix = df.corr()\n",
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(corr_matrix,\n",
    "                 annot=True,\n",
    "                 linewidth=0.5,\n",
    "                 fmt=\".2f\",\n",
    "                 color=[\"Yellow\",\"Green\",\"Blue\"]);\n",
    "# bottom, top = ax.get_ylim()\n",
    "# ax.set_ylim(bottom+0.5, top-0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = df.drop(\"target\",axis = 1)\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test Split\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got our data split into training & test sets, it's time to build a machine learning model.\n",
    "\n",
    "We'll train it (find the patterns on the training set)\n",
    "\n",
    "And we'll test it (use the patterns on the test set)\n",
    "\n",
    "We're going to use 3 different machine learning models:\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors Classifier\n",
    "3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary\n",
    "models = {\"Logistic Regression\": LogisticRegression(),\n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"Random Forest\": RandomForestClassifier()}\n",
    "\n",
    "# Create a function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models : a dict of different scikit-learn machine learning models\n",
    "    X_train : training data (no labels)\n",
    "    X_test : testing data (no labels)\n",
    "    y_train : training labels\n",
    "    y_test : test labels\n",
    "    \"\"\"\n",
    "    # Set randm seed\n",
    "    np.random.seed(42)\n",
    "    # Make a dictionary to keep model scores\n",
    "    model_scores={}\n",
    "    # Loop through models\n",
    "    for name,model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train,y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name]=model.score(X_test,y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = fit_and_score(models=models,\n",
    "                             X_train=X_train,\n",
    "                             X_test=X_test,\n",
    "                             y_train=y_train,          \n",
    "                             y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index = [\"accuracy\"])\n",
    "model_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare.T.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n",
    "\n",
    "Let's look at the following:\n",
    "* Hyperparameter Tuning\n",
    "* Feature importance\n",
    "* Confusion Matrix\n",
    "* Cross-validation\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "* Classification report\n",
    "* ROC Curve\n",
    "* Area under the curve (AUC)\n",
    "\n",
    "#### HyperParameter Tuning (by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune KNN\n",
    "\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1,21)\n",
    "\n",
    "# Setup KNN instance\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different n_Neighbors\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors=i)\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Update the training scores list\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    \n",
    "    # Update the test scores list\n",
    "    test_scores.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors, train_scores, label = \"Train score\")\n",
    "plt.plot(neighbors, test_scores, label = \"Test score\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model Scores\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN Score on the test data: {max(test_scores)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning by RandomizedSearchCV\n",
    "\n",
    "We're going to tune:\n",
    "* Logistic Regression\n",
    "* Random Forest Classifier\n",
    "\n",
    "...using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hyperparametr grid for LogisticRegression\n",
    "log_reg_grid = {\"C\": np.logspace(-4,4,20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Create a hyperparameter grid for RandomForestClassifer\n",
    "rf_grid = {\"n_estimators\": np.arange(10,1000,50),\n",
    "           \"max_depth\": [None,3,5,10],\n",
    "           \"min_samples_split\": np.arange(2,20,2),\n",
    "           \"min_samples_leaf\": np.arange(1,20,2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got hyperparameter grids setup for each of our models, let's tune them using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Logistic Regression\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup Random hyperparameter search for Logistic Regression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "# Fit Random Hyperparameter search model for Logistic Regression\n",
    "rs_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup Random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=20,\n",
    "                           verbose=True)\n",
    "# Fit Random Hyperparameter search model for RandomForestClassifier\n",
    "rs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameter\n",
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the randomized Search Random Forest Classifier Model\n",
    "rs_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Since our Logistic Regression model provides the best scores so far, we'll try and improve them again using GridSearchCV..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different hyperparameters for our LogisticRegression Model\n",
    "log_reg_grid = {\"C\": np.logspace(-4,4,30),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Setup grid hyperparameter search for Logistic Regresion\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid= log_reg_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best hyperparameters\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the grid search Logistic Regression model\n",
    "gs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our tuned machine learning classifier, beyond accuracy\n",
    "\n",
    "* ROC curve and AUC score\n",
    "* Confusion Matrix\n",
    "* Classification Report\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "...and it would be great if cross-validation was used where possible.\n",
    "\n",
    "To make comparisons and evaluate our trained model, first we need to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with tuned model\n",
    "y_preds = gs_log_reg.predict(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(gs_log_reg,X_test,y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_probs = gs_log_reg.predict_proba(X_test)\n",
    "y_probs_positive = y_probs[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "def plot_roc(fpr,tpr):\n",
    "    plt.plot(fpr,tpr,color=\"blue\",label=\"ROC\")\n",
    "    plt.xlabel(\"False positive rate (fpr)\")\n",
    "    plt.ylabel(\"True positive rate (tpr)\")\n",
    "    plt.title(\"Receiver Operating Characteristics (ROC) Curve\")\n",
    "    plt.legend()\n",
    "plot_roc(fpr,tpr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test,y_probs_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "\n",
    "def plot_conf_mat(conf_mat):\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax = sns.heatmap(conf_mat,\n",
    "                     annot=True,\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"True label\")\n",
    "    plt.ylabel(\"Predicted label\")\n",
    "#     bottom,top = ax.get_ylim()\n",
    "#     ax.set_ylim(bottom+0.5,top-0.5)\n",
    "    \n",
    "plot_conf_mat(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate evaluation metrics using cross validation\n",
    "\n",
    "We're going to calculate accuracy, precision, recall and f1-score of our model using cross-validation and to do so we;ll be using `cross_val_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check best hyperparameters\n",
    "\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier with best parameters\n",
    "clf = LogisticRegression(C= 0.20433597178569418,\n",
    "                         solver= 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated accuracy:\n",
    "cv_acc = cross_val_score(clf,\n",
    "                         X,\n",
    "                         y,\n",
    "                         cv=5,\n",
    "                         scoring=\"accuracy\")\n",
    "np.mean(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated precision:\n",
    "cv_precision = cross_val_score(clf,\n",
    "                               X,\n",
    "                               y,\n",
    "                               cv=5,\n",
    "                               scoring=\"precision\")\n",
    "np.mean(cv_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated recall:\n",
    "cv_recall = cross_val_score(clf,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=5,\n",
    "                            scoring=\"recall\")\n",
    "np.mean(cv_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated f1:\n",
    "cv_f1 = cross_val_score(clf,\n",
    "                        X,\n",
    "                        y,\n",
    "                        cv=5,\n",
    "                        scoring=\"f1\")\n",
    "np.mean(cv_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validated metrics\n",
    "\n",
    "cv_metrics = pd.DataFrame({\"Accuracy\": np.mean(cv_acc),\n",
    "                           \"Precision\": np.mean(cv_precision),\n",
    "                           \"Recall\": np.mean(cv_recall),\n",
    "                           \"F1\": np.mean(cv_f1)},\n",
    "                           index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics.T.plot.barh(title=\"Cross-validated classification matrix\",legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "\n",
    "It is another way of asking which features contributed most to the outcomes of the model and how did they contribute?\n",
    "\n",
    "Finding feature importance is different for each machine learning model. One way to find feature importance is to search for (MODEL NAME) feature importance.\n",
    "\n",
    "Let's find the feature importance for our Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an instance of LogisticRegressison\n",
    "gs_log_reg.best_params_\n",
    "\n",
    "clf = LogisticRegression(C= 0.20433597178569418, solver= 'liblinear')\n",
    "\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coef_\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match coef's of features to columns\n",
    "feature_dict = dict(zip(df.columns,clf.coef_[0]))\n",
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "feature_df = pd.DataFrame(feature_dict,index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.T.plot.barh(title=\"Feature Importance\", legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.sex,df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.slope,df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experimentation\n",
    "\n",
    "If you haven't hit your evaluation metric yet...ask yourself...\n",
    "\n",
    "* Could you collect more data?\n",
    "* Could you try a better model? Like CatBoost or XGBoost?\n",
    "* Could we improve the current models? (beyond what we've done so far)\n",
    "* If your model is good enough (you have hit your evaluation metric) how would you export it and share it with others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump,load\n",
    "\n",
    "dump(gs_log_reg,filename=\"best-model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load(\"best-model.joblib\")\n",
    "best_model.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
