{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEB SCRAPING:-**\n",
    "\n",
    "Web Scraping also known as Screen Scraping, Web Data Extraction, Web Harvesting etc is a technique emoloyed to extract large amounts of data from websites whereby the data is extracted and saved to a local file in your computer.\n",
    "\n",
    "* BeautifulSoup is a Python Library for pulling data out of HTML and XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.tgcindia.com/blog/'\n",
    "data = requests.get(url)\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in soup.find('p'):\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in soup.find_all('p'):\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in soup.find_all('p'):\n",
    "    print(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in soup.find_all('h5'):\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for links in soup.find_all('a'):\n",
    "    link = links.get(\"href\")\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for links in soup.find_all('a'):\n",
    "    link = links.get(\"href\")\n",
    "    if link!='#' and link!='#login' and link!=None:\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEB SCRAPING WITH REGEX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://www.tgcindia.com/blog/\")\n",
    "print(res)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find out all links\n",
    "# <a href= ' matches the characters <a href= ' literally (case sensitive)\n",
    "# \" matches the character \" literally (case sensitive)\n",
    "# 1st Capturing Group (.+?) .+? matches any character (except for line terminators)\n",
    "# +? Quantifier — Matches between one and unlimited times, as few times as possible, expanding as needed (lazy)\n",
    "# \" matches the character \" literally (case sensitive)\n",
    "# 'target matches the characters 'target literally (case sensitive)\n",
    "def find_links(page_source):\n",
    "    regex = re.compile(r\"<a href=\\\"(.+?)\\\" target\")\n",
    "    result = regex.findall(page_source)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = find_links(res.text)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find out top 5 links\n",
    "top_links = links[:5]\n",
    "print(top_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to open top 5 links in Web Browser automatically\n",
    "import webbrowser\n",
    "for link in top_links:\n",
    "    webbrowser.open(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web Scraping with Google**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, webbrowser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter something to search: \")\n",
    "print('Ruko Zara Sabr Karo....')\n",
    "\n",
    "google_search = requests.get(\"https://www.google.com/search?q=\"+user_input)\n",
    "\n",
    "soup = BeautifulSoup(google_search.text, 'html.parser')\n",
    "search_results = soup.find_all('a')\n",
    "print(search_results[0:5])\n",
    "for link in search_results[:5]:\n",
    "    actual_link = link.get('href')\n",
    "    print(actual_link)\n",
    "    if actual_link[0:3] == \"/se\":\n",
    "        webbrowser.open('https://www.google.com'+actual_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">NEWS API SCRAPING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urlr\n",
    "req = 'http://newsapi.org/v2/top-headlines?country=in&category=entertainment&apiKey=6964ff88692444eaa57bf7da1f69d60f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = urlr.urlopen(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw.read()) # Data shown in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myjson = raw.read()\n",
    "data = json.loads(myjson.decode()) # decode() converts data in json format to string format\n",
    "print(data['articles'][0]['title']) # json.loads() accept only string format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Headline \" + str(i+1) + \" is :\")\n",
    "    print(data['articles'][i]['title'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Description \" + str(i+1) + \" is :\")\n",
    "    print(data['articles'][i]['description'])\n",
    "    print(\"\\nContent is :\")\n",
    "    print(data['articles'][i]['content'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['articles'])):\n",
    "    print(\"Headline \" + str(i+1) + \" is :\")\n",
    "    print(data['articles'][i]['title'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install modulename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Flipkart Samsung Phone Scraping</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.flipkart.com/search?q=samsung+mobiles&sid=tyy%2C4io&as=on&as-show=on&otracker=AS_QueryStore_OrganicAutoSuggest_1_7_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_7_na_na_na&as-pos=1&as-type=RECENT&suggestionId=samsung+mobiles%7CMobiles&requestId=4a54d4d5-a7f3-47b8-abef-47f664996a03&as-backfill=on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = urlopen(link)\n",
    "data_html = req.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data_html,'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile = soup.find_all('div', attrs={'class':'_3O0U0u'})\n",
    "print(len(mobile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOBILES = [['Name','Rating','Price']]\n",
    "for item in mobile:\n",
    "    name = item.find_all('div',attrs = {'class':'_3wU53n'})\n",
    "    name = name[0]\n",
    "    name = name.text\n",
    "    name = name.split('-')\n",
    "    final_name = name[0]\n",
    "    \n",
    "    price = item.find_all('div',attrs={'class':'_1vC4OE'})\n",
    "    price = price[0].text\n",
    "    price = price.replace('₹','')\n",
    "    price = price.replace(',','')\n",
    "    \n",
    "    rating = item.find_all('div',attrs={'class':'hGSR34'})\n",
    "    rating = rating[0].text\n",
    "    if rating:\n",
    "        MOBILES.append([final_name,rating,price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MOBILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate(MOBILES,tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">ESPN CRICINFO SCRAPING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.espncricinfo.com/rankings/content/page/211271.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = soup.find_all('h3')\n",
    "headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in headings:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for i in headings:\n",
    "    h.append(i.text.strip())\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = [i.text.strip() for i in headings]\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all('table',attrs={'class':'StoryengineTable'})\n",
    "print(len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table[0]\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = table[0]\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = t1.find_all('tr')\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(rows)):\n",
    "    if i == 0:\n",
    "        column = rows[i].find_all('th')\n",
    "    else:\n",
    "        column = rows[i].find_all('td')\n",
    "    data.append([x.text.strip() for x in column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(h2[0])\n",
    "print(tabulate(data,tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">QUANDL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
